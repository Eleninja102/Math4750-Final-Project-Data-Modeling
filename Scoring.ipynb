{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, classification_report, confusion_matrix, r2_score\n",
    "\n",
    "\n",
    "from sklearn_evaluation.plot import grid_search  # For plotting results from grid search\n",
    "from tabulate import tabulate\n",
    "\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "import joblib\n",
    "# Data handling and manipulation\n",
    "import pandas as pd  # Data analysis and manipulation tool\n",
    "import numpy as np  # Numerical operations on arrays and matrices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = joblib.load('./Data/gridSearch_rf.pickle')\n",
    "balanced_model = joblib.load('./Data/gridSearch_rf_balanced.pickle')\n",
    "imbalanced_model = joblib.load('./Data/gridSearch_rf_imbalanced.pickle')\n",
    "\n",
    "best_models: dict = {\"base\": base_model.best_estimator_, \"balanced\": balanced_model.best_estimator_, \"imbalanced\": imbalanced_model.best_estimator_}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_pickle('Data/X_test.pickle')\n",
    "y_test = pd.read_pickle('Data/y_test.pickle')\n",
    "X_train = pd.read_pickle('Data/X_train.pickle')\n",
    "y_train= pd.read_pickle('Data/y_train.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Data Model: base  Geometric Mean: 0.7149113728705784\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| Metric       |   precision |   recall |   f1-score |   support |\n",
      "+==============+=============+==========+============+===========+\n",
      "| 0            |    0.932919 | 0.992948 |   0.961998 |     13614 |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| 1            |    0.914818 | 0.514728 |   0.658786 |      2003 |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| accuracy     |    0.931613 |          |            |           |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| macro avg    |    0.923869 | 0.753838 |   0.810392 |     15617 |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| weighted avg |    0.930598 | 0.931613 |   0.923109 |     15617 |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "\n",
      "\n",
      "Test Data Model: balanced  Geometric Mean: 0.8092142691682905\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| Metric       |   precision |   recall |   f1-score |   support |\n",
      "+==============+=============+==========+============+===========+\n",
      "| 0            |    0.962337 | 0.844572 |   0.899617 |     13614 |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| 1            |    0.423276 | 0.775337 |   0.547602 |      2003 |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| accuracy     |    0.835692 |          |            |           |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| macro avg    |    0.692806 | 0.809954 |   0.723609 |     15617 |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| weighted avg |    0.893198 | 0.835692 |   0.854468 |     15617 |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "\n",
      "\n",
      "Test Data Model: imbalanced  Geometric Mean: 0.8145372498555367\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| Metric       |   precision |   recall |   f1-score |   support |\n",
      "+==============+=============+==========+============+===========+\n",
      "| 0            |    0.964854 | 0.836859 |   0.89631  |     13614 |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| 1            |    0.416907 | 0.792811 |   0.546456 |      2003 |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| accuracy     |    0.83121  |          |            |           |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| macro avg    |    0.690881 | 0.814835 |   0.721383 |     15617 |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| weighted avg |    0.894576 | 0.83121  |   0.851439 |     15617 |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for key, model in best_models.items():\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    report = classification_report(y_test, y_test_pred, output_dict=True)  # Output as a dictionary\n",
    "\n",
    "    # Convert the dictionary into rows for the table\n",
    "    rows = []\n",
    "    for metric, values in report.items():\n",
    "        if isinstance(values, dict):  # Handle metrics like precision, recall, f1-score\n",
    "            rows.append([metric] + list(values.values()))\n",
    "        else:  # Handle single metrics like accuracy\n",
    "            rows.append([metric, values])\n",
    "            \n",
    "    # Create headers: dynamically based on dictionary keys\n",
    "    headers = [\"Metric\"] + (list(report[\"weighted avg\"].keys()) if \"weighted avg\" in report else [])\n",
    "\n",
    "    print(f\"Test Data Model: {key}  Geometric Mean: {geometric_mean_score(y_test, y_test_pred)}\")\n",
    "    print(tabulate(rows, headers=headers, tablefmt=\"grid\"))  # Use \"grid\" format for better visuals\n",
    "    print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Model: base  Geometric Mean: 0.7046304342072924\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| Metric       |   precision |   recall |   f1-score |   support |\n",
      "+==============+=============+==========+============+===========+\n",
      "| 0            |    0.934536 | 0.993399 |   0.963069 |     54838 |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| 1            |    0.913293 | 0.499803 |   0.646052 |      7629 |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| accuracy     |    0.933117 |          |            |           |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| macro avg    |    0.923915 | 0.746601 |   0.804561 |     62467 |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| weighted avg |    0.931942 | 0.933117 |   0.924352 |     62467 |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "\n",
      "\n",
      "Train Data Model: balanced  Geometric Mean: 0.8072605103076389\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| Metric       |   precision |   recall |   f1-score |   support |\n",
      "+==============+=============+==========+============+===========+\n",
      "| 0            |    0.964104 | 0.840931 |   0.898315 |     54838 |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| 1            |    0.403963 | 0.774938 |   0.531082 |      7629 |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| accuracy     |    0.832872 |          |            |           |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| macro avg    |    0.684033 | 0.807935 |   0.714698 |     62467 |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| weighted avg |    0.895694 | 0.832872 |   0.853465 |     62467 |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "\n",
      "\n",
      "Train Data Model: imbalanced  Geometric Mean: 0.8100823209475905\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| Metric       |   precision |   recall |   f1-score |   support |\n",
      "+==============+=============+==========+============+===========+\n",
      "| 0            |    0.965667 | 0.833984 |   0.895008 |     54838 |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| 1            |    0.397365 | 0.786866 |   0.528061 |      7629 |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| accuracy     |    0.828229 |          |            |           |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| macro avg    |    0.681516 | 0.810425 |   0.711534 |     62467 |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| weighted avg |    0.896261 | 0.828229 |   0.850193 |     62467 |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for key, model in best_models.items():\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    report = classification_report(y_train, y_train_pred, output_dict=True)  # Output as a dictionary\n",
    "\n",
    "    # Convert the dictionary into rows for the table\n",
    "    rows = []\n",
    "    for metric, values in report.items():\n",
    "        if isinstance(values, dict):  # Handle metrics like precision, recall, f1-score\n",
    "            rows.append([metric] + list(values.values()))\n",
    "        else:  # Handle single metrics like accuracy\n",
    "            rows.append([metric, values])\n",
    "            \n",
    "    # Create headers: dynamically based on dictionary keys\n",
    "    headers = [\"Metric\"] + (list(report[\"weighted avg\"].keys()) if \"weighted avg\" in report else [])\n",
    "\n",
    "    print(f\"Train Data Model: {key}  Geometric Mean: {geometric_mean_score(y_train, y_train_pred)}\")\n",
    "    print(tabulate(rows, headers=headers, tablefmt=\"grid\"))  # Use \"grid\" format for better visuals\n",
    "    print(\"\\n\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
