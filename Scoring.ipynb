{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, classification_report, confusion_matrix, r2_score\n",
    "\n",
    "\n",
    "from sklearn_evaluation.plot import grid_search  # For plotting results from grid search\n",
    "from tabulate import tabulate\n",
    "\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "import joblib\n",
    "# Data handling and manipulation\n",
    "import pandas as pd  # Data analysis and manipulation tool\n",
    "import numpy as np  # Numerical operations on arrays and matrices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = joblib.load('./Data/gridSearch_rf.pickle')\n",
    "balanced_model = joblib.load('./Data/gridSearch_rf_balanced.pickle')\n",
    "imbalanced_model = joblib.load('./Data/gridSearch_rf_imbalanced.pickle')\n",
    "\n",
    "best_models: dict = {\"base\": base_model.best_estimator_, \"balanced\": balanced_model.best_estimator_, \"imbalanced\": imbalanced_model.best_estimator_}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_pickle('Data/X_test.pickle')\n",
    "y_test = pd.read_pickle('Data/y_test.pickle')\n",
    "X_train = pd.read_pickle('Data/X_train.pickle')\n",
    "y_train= pd.read_pickle('Data/y_train.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Data Model: base  Geometric Mean: 0.6629396740699007\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| Metric       |   precision |   recall |   f1-score |   support |\n",
      "+==============+=============+==========+============+===========+\n",
      "| 0            |    0.931778 | 0.994691 |   0.962207 |     41440 |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| 1            |    0.915677 | 0.441835 |   0.596058 |      5407 |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| accuracy     |    0.930881 |          |            |           |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| macro avg    |    0.923727 | 0.718263 |   0.779133 |     46847 |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| weighted avg |    0.92992  | 0.930881 |   0.919947 |     46847 |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "\n",
      "\n",
      "Test Data Model: balanced  Geometric Mean: 0.7880439262304331\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| Metric       |   precision |   recall |   f1-score |   support |\n",
      "+==============+=============+==========+============+===========+\n",
      "| 0            |    0.961126 | 0.839455 |   0.89618  |     41440 |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| 1            |    0.375481 | 0.739782 |   0.498132 |      5407 |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| accuracy     |    0.827951 |          |            |           |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| macro avg    |    0.668304 | 0.789618 |   0.697156 |     46847 |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| weighted avg |    0.893532 | 0.827951 |   0.850238 |     46847 |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "\n",
      "\n",
      "Test Data Model: imbalanced  Geometric Mean: 0.7924620935422505\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| Metric       |   precision |   recall |   f1-score |   support |\n",
      "+==============+=============+==========+============+===========+\n",
      "| 0            |    0.963537 | 0.825772 |   0.889351 |     41440 |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| 1            |    0.362866 | 0.760496 |   0.491308 |      5407 |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| accuracy     |    0.818238 |          |            |           |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| macro avg    |    0.663201 | 0.793134 |   0.690329 |     46847 |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| weighted avg |    0.894208 | 0.818238 |   0.843409 |     46847 |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for key, model in best_models.items():\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    report = classification_report(y_test, y_test_pred, output_dict=True)  # Output as a dictionary\n",
    "\n",
    "    # Convert the dictionary into rows for the table\n",
    "    rows = []\n",
    "    for metric, values in report.items():\n",
    "        if isinstance(values, dict):  # Handle metrics like precision, recall, f1-score\n",
    "            rows.append([metric] + list(values.values()))\n",
    "        else:  # Handle single metrics like accuracy\n",
    "            rows.append([metric, values])\n",
    "            \n",
    "    # Create headers: dynamically based on dictionary keys\n",
    "    headers = [\"Metric\"] + (list(report[\"weighted avg\"].keys()) if \"weighted avg\" in report else [])\n",
    "\n",
    "    print(f\"Test Data Model: {key}  Geometric Mean: {geometric_mean_score(y_test, y_test_pred)}\")\n",
    "    print(tabulate(rows, headers=headers, tablefmt=\"grid\"))  # Use \"grid\" format for better visuals\n",
    "    print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Model: base  Geometric Mean: 0.6699044446931706\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| Metric       |   precision |   recall |   f1-score |   support |\n",
      "+==============+=============+==========+============+===========+\n",
      "| 0            |    0.93214  | 0.99463  |   0.962372 |    165545 |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| 1            |    0.917256 | 0.451195 |   0.604861 |     21842 |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| accuracy     |    0.931287 |          |            |           |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| macro avg    |    0.924698 | 0.722912 |   0.783616 |    187387 |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| weighted avg |    0.930405 | 0.931287 |   0.9207   |    187387 |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "\n",
      "\n",
      "Train Data Model: balanced  Geometric Mean: 0.7901590994106724\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| Metric       |   precision |   recall |   f1-score |   support |\n",
      "+==============+=============+==========+============+===========+\n",
      "| 0            |    0.96111  | 0.841536 |   0.897357 |    165545 |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| 1            |    0.381851 | 0.741919 |   0.5042   |     21842 |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| accuracy     |    0.829924 |          |            |           |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| macro avg    |    0.671481 | 0.791727 |   0.700779 |    187387 |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| weighted avg |    0.893591 | 0.829924 |   0.85153  |    187387 |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "\n",
      "\n",
      "Train Data Model: imbalanced  Geometric Mean: 0.7938789644726154\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| Metric       |   precision |   recall |   f1-score |   support |\n",
      "+==============+=============+==========+============+===========+\n",
      "| 0            |    0.963229 | 0.829164 |   0.891183 |    165545 |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| 1            |    0.369895 | 0.760095 |   0.497625 |     21842 |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| accuracy     |    0.821114 |          |            |           |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| macro avg    |    0.666562 | 0.79463  |   0.694404 |    187387 |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| weighted avg |    0.89407  | 0.821114 |   0.845309 |    187387 |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for key, model in best_models.items():\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    report = classification_report(y_train, y_train_pred, output_dict=True)  # Output as a dictionary\n",
    "\n",
    "    # Convert the dictionary into rows for the table\n",
    "    rows = []\n",
    "    for metric, values in report.items():\n",
    "        if isinstance(values, dict):  # Handle metrics like precision, recall, f1-score\n",
    "            rows.append([metric] + list(values.values()))\n",
    "        else:  # Handle single metrics like accuracy\n",
    "            rows.append([metric, values])\n",
    "            \n",
    "    # Create headers: dynamically based on dictionary keys\n",
    "    headers = [\"Metric\"] + (list(report[\"weighted avg\"].keys()) if \"weighted avg\" in report else [])\n",
    "\n",
    "    print(f\"Train Data Model: {key}  Geometric Mean: {geometric_mean_score(y_train, y_train_pred)}\")\n",
    "    print(tabulate(rows, headers=headers, tablefmt=\"grid\"))  # Use \"grid\" format for better visuals\n",
    "    print(\"\\n\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
